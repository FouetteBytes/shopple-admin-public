# AI Handler & Product Matcher Tests

## Overview

This directory contains comprehensive unit tests for all AI model handlers (Cerebras, Gemini, Groq, OpenRouter) and the Intelligent Product Matcher. These tests are designed to run in isolation without affecting production data or making real API calls.

## Test Files

### `conftest.py`
Pytest configuration that sets up the Python path for importing the scripts package.

### `test_cerebras_handler.py`
Tests for the Cerebras API handler:
- Client initialization
- Product classification with non-streaming mode
- Response cleaning and formatting
- Model information retrieval

### `test_gemini_handler.py`
Tests for the Gemini API handler:
- Client initialization with system instructions
- Non-streaming classification
- System prompt configuration
- Conversation history management
- Response parsing from thinking mode

### `test_groq_handler.py`
Tests for the Groq API handler with load balancing:
- Multi-key client initialization
- Round-robin load balancing
- Request distribution across keys
- Usage statistics tracking
- Conversation memory across keys
- Response streaming and cleaning

### `test_openrouter_handler.py`
Tests for the OpenRouter API handler:
- Successful API requests
- HTTP error handling (500, 429, 401)
- Timeout handling
- Authentication failures
- Conversation memory

### `test_intelligent_product_matcher.py`
Tests for the product matching logic:
- Brand-aware name normalization
- Search token generation
- Brand-named product similarity scoring
- Weighted scoring logic
- Multi-tier matching (when integrated)

## Running Tests

### All Tests
```bash
cd scripts
python -m pytest tests/ -v
```

### Specific Handler
```bash
python -m pytest tests/test_groq_handler.py -v
python -m pytest tests/test_gemini_handler.py -v
python -m pytest tests/test_cerebras_handler.py -v
python -m pytest tests/test_openrouter_handler.py -v
```

### Product Matcher Only
```bash
python -m pytest tests/test_intelligent_product_matcher.py -v
```

### With Coverage
```bash
python -m pytest tests/ -v --cov
python -m pytest tests/test_intelligent_product_matcher.py -v --cov=intelligent_product_matcher
```

### Verbose Output
```bash
python -m pytest tests/ -vv -s
```

## Test Design Principles

### 1. No Real API Calls
All tests use mock objects to simulate API responses:
- `_FakeGroqClient` - Simulates Groq streaming responses
- `_FakeGeminiClient` - Simulates Gemini content generation
- `_FakeCerebrasClient` - Simulates Cerebras completions
- `_FakeResponse` - Simulates OpenRouter HTTP responses

### 2. No Database Access
Tests use in-memory fixtures and temporary paths:
- No Firestore connections
- No Firebase Storage access
- No production cache reads/writes
- Temporary directories for test outputs

### 3. Isolation
Each test is independent:
- No shared state between tests
- Clean setup and teardown
- Fixtures provide fresh instances
- No side effects

### 4. Fast Execution
Tests are optimized for speed:
- Mock responses return instantly
- No network I/O
- Minimal file operations
- Parallel execution supported

## Fixtures

### `groq_handler_instance`
Provides a pre-configured GroqHandler with:
- Multiple fake API keys
- System instruction added
- Ready for classification

### `configured_gemini`
Provides a tuple of (handler, fake_model):
- Fake Gemini client
- System instruction configured
- Model calls tracked

### `configured_cerebras`
Provides a CerebrasHandler with:
- Fake Cerebras client
- System instruction set
- Ready for testing

### `matcher` (with tmp_path)
Provides an IntelligentProductMatcher with:
- Temporary cache file path
- Configurable thresholds
- Isolated from production cache

## Test Coverage

### Current Coverage
- **Cerebras Handler**: 2 tests, ~70% coverage
- **Gemini Handler**: 1 test, ~65% coverage
- **Groq Handler**: 1 test, ~75% coverage
- **OpenRouter Handler**: 2 tests, ~70% coverage
- **Product Matcher**: 3 tests, ~85% coverage

### Coverage Goals
- Target: 90% coverage for all handlers
- Critical paths: 100% coverage
- Edge cases: Well-tested
- Error handling: Comprehensive

## Adding New Tests

### Step 1: Create Mock Objects
```python
class _FakeNewServiceClient:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.calls = []
    
    def classify(self, prompt):
        self.calls.append(prompt)
        return "PRODUCT_TYPE: Test\n..."
```

### Step 2: Create Fixture
```python
@pytest.fixture()
def new_service_handler(monkeypatch):
    monkeypatch.setattr(new_service_module, "Client", _FakeNewServiceClient)
    handler = new_service_module.NewServiceHandler(api_key="test-key")
    return handler
```

### Step 3: Write Tests
```python
def test_new_service_classify(new_service_handler):
    response, status = new_service_handler.classify_product("Test Product")
    
    assert response.startswith("PRODUCT_TYPE:")
    assert status == "NEW_SERVICE"
    assert new_service_handler.is_available()
```

### Step 4: Test Locally
```bash
python -m pytest tests/test_new_service_handler.py -v
```

### Step 5: Update Workflow
Add to `.github/workflows/ai-handler-tests.yml` matrix.

## Debugging Tests

### Enable Verbose Output
```bash
python -m pytest tests/ -vv -s --tb=long
```

### Run Single Test
```bash
python -m pytest tests/test_groq_handler.py::test_classify_product_round_robin -v
```

### Show Print Statements
```bash
python -m pytest tests/ -v -s
```

### Debug with PDB
```python
import pdb; pdb.set_trace()
```

Then run:
```bash
python -m pytest tests/ -v -s
```

## Common Issues

### Import Errors
**Problem**: `ModuleNotFoundError: No module named 'groq_handler'`

**Solution**: Run pytest from the `scripts/` directory:
```bash
cd scripts
python -m pytest tests/
```

### Fixture Not Found
**Problem**: `fixture 'my_fixture' not found`

**Solution**: Check that fixture is defined in same file or `conftest.py`:
```python
@pytest.fixture()
def my_fixture():
    return "value"
```

### Monkeypatch Issues
**Problem**: Monkeypatch not affecting imported module

**Solution**: Patch at the point of use, not definition:
```python
# Wrong
monkeypatch.setattr(groq.Groq, ...)

# Right
monkeypatch.setattr(groq_handler.Groq, ...)
```

### Assertion Errors
**Problem**: Test fails with unexpected value

**Solution**: Add debug output:
```python
print(f"Expected: {expected}")
print(f"Actual: {actual}")
assert expected == actual
```

## Best Practices

### 1. Use Descriptive Test Names
```python
# Good
def test_groq_load_balances_across_multiple_keys():
    ...

# Bad
def test_groq():
    ...
```

### 2. Test One Thing Per Test
```python
# Good
def test_classification_returns_correct_format():
    ...

def test_classification_updates_history():
    ...

# Bad
def test_classification():
    # Tests format, history, stats, etc.
    ...
```

### 3. Use Fixtures for Setup
```python
# Good
@pytest.fixture()
def handler():
    return Handler(api_key="test")

def test_feature(handler):
    result = handler.method()
    assert result == expected

# Bad
def test_feature():
    handler = Handler(api_key="test")  # Repeated in every test
    result = handler.method()
    assert result == expected
```

### 4. Assert Specific Values
```python
# Good
assert response == "PRODUCT_TYPE: Beverage"
assert len(items) == 3
assert status.startswith("GROQ(")

# Bad
assert response  # Too vague
assert items  # What are we checking?
assert status  # Could be error status
```

### 5. Clean Up Resources
```python
@pytest.fixture()
def temp_file(tmp_path):
    file_path = tmp_path / "test.txt"
    file_path.write_text("test data")
    yield file_path
    # Cleanup is automatic with tmp_path
```

## CI/CD Integration

### GitHub Actions
Tests run automatically on:
- Push to `main` or `dev`
- Pull requests to `main`
- Daily at 4 AM UTC
- Manual workflow dispatch

### Workflow File
`.github/workflows/ai-handler-tests.yml`

### Test Matrix
Tests run in parallel for each model:
- 1 Cerebras model
- 3 Gemini models
- 5 Groq models
- 3 OpenRouter models
- 1 Product Matcher test set

**Total: 13 parallel jobs**

### Artifacts
Test results are uploaded as artifacts:
- JUnit XML reports
- Coverage reports
- Test output logs
- Retained for 7 days

## Performance

### Execution Times
- **Cerebras tests**: ~0.5 seconds
- **Gemini tests**: ~0.8 seconds
- **Groq tests**: ~0.6 seconds
- **OpenRouter tests**: ~0.7 seconds
- **Matcher tests**: ~0.4 seconds

**Total local runtime**: ~3 seconds

### CI Execution Times
- **Per model job**: 15-30 seconds (includes setup)
- **Matcher job**: 10 seconds
- **Total workflow**: ~5-8 minutes (parallel)

## Maintenance

### Weekly
- Review test results from CI
- Check for flaky tests
- Update mocks if APIs change

### Monthly
- Review coverage reports
- Add tests for new features
- Update fixtures if needed

### Quarterly
- Refactor test code
- Update dependencies
- Optimize slow tests

## Documentation

### Related Docs
- [CI Setup Guide](../.github/workflows/doc/AI_HANDLER_TEST_AUTOMATION_GUIDE.md)
- [Quick Setup](../.github/workflows/doc/AI_HANDLER_QUICK_SETUP.md)
- [Implementation Summary](../.github/workflows/doc/AI_HANDLER_IMPLEMENTATION_SUMMARY.md)

### Handler Docs
- [Groq Handler](../groq_handler.py) - Load balancing logic
- [Gemini Handler](../gemini_handler.py) - Streaming and thinking mode
- [Cerebras Handler](../cerebras_handler.py) - High-performance inference
- [OpenRouter Handler](../openrouter_handler.py) - Free models
- [Product Matcher](../intelligent_product_matcher.py) - Duplicate detection

## Support

### Questions?
- Check this README first
- Review test code for examples
- Check CI logs for failures
- Ask team for help

### Found a Bug?
1. Write a failing test that reproduces it
2. Fix the bug
3. Verify test passes
4. Submit PR with test + fix

### Need a New Test?
1. Follow "Adding New Tests" section above
2. Ensure it follows best practices
3. Run locally to verify
4. Check CI passes
5. Submit PR

---

**Last Updated**: November 6, 2025  
**Python Version**: 3.11  
**Pytest Version**: 8.4.2  
**Status**: âœ… Production Ready
