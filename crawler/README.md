# Shopple Multi-Site Crawler

An intelligent, AI-powered web crawler designed to extract product information from major supermarket websites (Keells Super, Cargills Online) using advanced scraping techniques and LLM-based data extraction.

## Running the Crawler

**⚠️ Important:** The crawler is designed to run inside a Docker container.
It requires a specific headless browser environment (Chromium) which is pre-configured in `Dockerfile.crawler`.

**To Start:**
```bash
# From the project root
docker-compose up --build crawler
```

### Logging
The crawler writes structured JSON logs to `logs/crawler.json.log`.
These logs are automatically collected by the **Fluent Bit** container and forwarded to **OpenSearch**.

## Directory Structure

```
crawler/
├── crawler_manager.py          # Main entry point for crawler jobs
├── enhanced_crawler_manager.py # Advanced manager with better error handling
├── crawler_integration.py      # Integration logic
├── file_watcher.py             # Watches for new output files
├── job_watcher.py              # Monitors crawler jobs
├── firebase_storage_manager.py # Manages Firebase Storage uploads
├── keells/                     # Keells Super crawlers
│   ├── keells_base_crawler.py  # Base class for Keells
│   └── *_crawler.py            # Category-specific crawlers (Fresh, Grocery, etc.)
├── cargills/                   # Cargills Online crawlers
│   ├── cargills_base_crawler.py # Base class for Cargills
│   └── *_crawler.py            # Category-specific crawlers
├── cache/                      # Local persistence
│   ├── sqlite_store.py         # SQLite database for run state
│   └── product_cache.json      # JSON cache
├── output/                     # Scraped data output
│   ├── keells/
│   └── cargills/
└── logs/                       # Crawler logs
```

## What This Crawler Does

This crawler performs automated product data extraction from:
1.  **Keells Super**: `https://www.keellssuper.com`
2.  **Cargills Online**: `https://cargillsonline.com`

It intelligently scrolls through product catalogs, loads items, and extracts structured product information including names, prices, and image URLs.

## Key Features

###  Intelligent Scrolling System
- **Adaptive Pagination**: Automatically detects and loads infinite scroll content
- **Smart Termination**: Stops scrolling when no new products are detected after multiple attempts
- **Product Count Monitoring**: Continuously tracks the number of loaded products to determine when the page is fully loaded
- **Configurable Limits**: Option to set maximum number of items to scrape or extract all available products

###  AI-Powered Data Extraction
- **LLM Integration**: Uses Groq's Llama-3.1-8b-instant model for intelligent content extraction
- **Structured Schema**: Employs Pydantic models to ensure consistent data structure
- **Robust Parsing**: Handles complex HTML structures and extracts clean, validated data

###  Two-Phase Architecture

#### Phase 1: Intelligent Loading
- Monitors page state and product count in real-time
- Performs strategic scrolling to trigger lazy-loading mechanisms
- Implements stability checks to ensure all content is loaded
- Provides detailed logging of the discovery process

#### Phase 2: Bulk Extraction
- Processes all loaded product cards in a single operation
- Applies CSS selectors to target specific product containers
- Uses LLM strategy for accurate data extraction from HTML content
- Validates and structures extracted data using Pydantic models

## Operational Components

### Crawler Manager (`crawler_manager.py`)
The central orchestrator that:
- Accepts crawl jobs from the API.
- Instantiates the appropriate crawler (Keells/Cargills).
- Manages the crawl lifecycle (start, stop, error handling).

### File Watcher (`file_watcher.py`)
A background service that monitors the `output/` directory. When a new JSON file is generated by a crawler:
1.  It detects the file close event.
2.  Triggers an upload to Firebase Storage via `firebase_storage_manager.py`.
3.  Updates the job status in the database.

### Job Watcher (`job_watcher.py`)
Monitors the status of active crawl jobs, ensuring they don't stall and handling timeouts or crashes.

### Firebase Storage Manager (`firebase_storage_manager.py`)
Handles the upload of scraped JSON files to Firebase Storage, organizing them by date and category for archival and processing.

## Technical Architecture

### Browser Automation
- **Crawl4AI Framework**: Modern async web crawling with JavaScript execution
- **Session Management**: Maintains persistent browser sessions for efficient scrolling
- **Stealth Mode**: Configurable headless/headed browser operation
- **Timeout Handling**: Robust error handling and retry mechanisms

### Data Processing Pipeline
1. **HTML Extraction**: Targets `.product-card-container` elements
2. **LLM Processing**: Sends HTML to language model for structured extraction
3. **Data Validation**: Uses Pydantic schemas to ensure data quality
4. **JSON Output**: Generates clean, structured JSON output

### Product Data Schema
Each extracted product contains:
- **Product Name**: Full product title (e.g., "Keells White Sugar 1kg")
- **Price**: Complete pricing information with currency (e.g., "Rs 297.00")
- **Image URL**: Direct link to product image from src attribute

## Configuration Options

### Environment Variables
- `GROQ_API_KEY`: Required API key for LLM-based extraction

### Scraping Parameters
- **Item Limits**: Set `max_items_to_scrape` to control extraction volume
- **Stability Threshold**: Configurable scroll stability detection
- **Timeout Settings**: Adjustable page load and processing timeouts

## Output Format

The crawler generates:
1. **Console Output**: Real-time progress and extracted data preview
2. **JSON File**: Complete product data saved to `products.json`
3. **Structured Data**: Validated and cleaned product information

## Intelligent Features

### Dynamic Content Detection
- Automatically adapts to infinite scroll implementations
- Detects when new content stops loading
- Handles dynamic JavaScript-rendered product cards

### Error Recovery
- Graceful handling of network timeouts
- Retry mechanisms for failed extractions
- Detailed error reporting and logging

### Performance Optimization
- Efficient session reuse for faster processing
- Bulk extraction to minimize API calls
- Configurable delays to respect website resources

## Use Cases

- **Price Monitoring**: Track product prices over time
- **Inventory Analysis**: Monitor product availability and catalog changes
- **Market Research**: Analyze grocery product trends and pricing
- **Data Integration**: Export structured data for external systems
- **Competitive Analysis**: Compare product offerings and pricing strategies

This crawler represents a sophisticated approach to e-commerce data extraction, combining modern web automation with AI-powered content processing to deliver accurate, structured product information at scale.
