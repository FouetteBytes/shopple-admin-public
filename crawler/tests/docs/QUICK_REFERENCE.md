# Quick Reference: Comprehensive Crawler Testing

## What's New? ğŸ‰

Instead of testing each crawler separately, we now have **ONE comprehensive test** that:
- Tests all 9 Keells crawlers in a single run
- Generates a unified Slack report showing results for each crawler
- Provides detailed metrics: duration, product count, file size, samples

## File Structure

```
crawler/
â”œâ”€â”€ keells/
â”‚   â”œâ”€â”€ keells_base_crawler.py          # â† Single source of truth (391 lines)
â”‚   â”œâ”€â”€ keells_beverages_crawler.py     # â† Simplified to 20 lines
â”‚   â”œâ”€â”€ keells_chilled_products_crawler.py
â”‚   â”œâ”€â”€ keells_frozen_food_crawler.py
â”‚   â”œâ”€â”€ keells_fruits_crawler.py
â”‚   â”œâ”€â”€ keells_groceries_crawler.py
â”‚   â”œâ”€â”€ keells_household_essentials_crawler.py
â”‚   â”œâ”€â”€ keells_meat_crawler.py
â”‚   â”œâ”€â”€ keells_seafood_crawler.py
â”‚   â””â”€â”€ keells_vegetables_crawler.py
â””â”€â”€ tests/
    â”œâ”€â”€ test_all_keells_crawlers.py     # â† NEW: Comprehensive test (442 lines)
    â””â”€â”€ test_keells_beverages.py        # â† Legacy: Single crawler test
```

## Run Comprehensive Test Locally

```bash
# Test all 9 crawlers (default: 100 items per crawler)
cd crawler/tests
pytest test_all_keells_crawlers.py::TestAllKeellsCrawlers::test_all_keells_crawlers -v -s

# Test with fewer items (faster)
MAX_ITEMS=10 pytest test_all_keells_crawlers.py::TestAllKeellsCrawlers::test_all_keells_crawlers -v -s

# Test with more items (comprehensive)
MAX_ITEMS=50 pytest test_all_keells_crawlers.py::TestAllKeellsCrawlers::test_all_keells_crawlers -v -s
```

## What You'll See

### Console Output
```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š Comprehensive Keells Crawler Test Results
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… PASSED: Beverages
   Products: 10 | Duration: 12.3s | Size: 45.2 KB
   Sample: Product 1, Product 2, Product 3

âœ… PASSED: Chilled Products
   Products: 10 | Duration: 11.8s | Size: 42.1 KB
   Sample: Product 1, Product 2, Product 3

... (7 more)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š OVERALL SUMMARY
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total Crawlers:  9
Successful:      9 âœ…
Failed:          0 âŒ
Total Products:  90
Total Duration:  120.5s
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Slack Notification (GitHub Actions)
When you push to GitHub, you'll get a **single Slack notification** showing:
- Overall summary (total crawlers, successful, failed, total products, total duration)
- Individual results for each of the 9 crawlers
- Status icons (âœ… success, âŒ failed)
- Link to view detailed logs

## GitHub Actions Workflow

### What Changed?

**Before:**
- Separate test for each crawler
- Multiple Slack notifications
- Harder to see overall status

**After:**
- Single comprehensive test job: `test-all-keells-crawlers`
- Single notification job: `notify-comprehensive`
- One Slack message with all results

### Workflow Jobs:

1. **test-all-keells-crawlers**
   - Runs all 9 crawlers with MAX_ITEMS=10 (90 total products)
   - Saves results to `test_results_summary.json`
   - Duration: ~2-3 minutes

2. **notify-comprehensive**
   - Parses test results
   - Builds comprehensive Slack message
   - Sends single notification with all crawler results

## Output Files

### Individual Crawler Outputs
Each crawler generates a timestamped file:
```
crawler/test_output/keells/
â”œâ”€â”€ beverages/
â”‚   â””â”€â”€ keells_beverages_20250103_103000.json
â”œâ”€â”€ chilled_products/
â”‚   â””â”€â”€ keells_chilled_products_20250103_103012.json
â”œâ”€â”€ frozen_food/
â”‚   â””â”€â”€ keells_frozen_food_20250103_103025.json
... (6 more)
```

### Comprehensive Test Summary
```json
crawler/test_output/keells/test_results_summary.json

{
  "summary": {
    "total_crawlers": 9,
    "successful": 9,
    "failed": 0,
    "total_products": 90,
    "total_duration": 120.5
  },
  "results": {
    "beverages": {
      "status": "success",
      "product_count": 10,
      "duration": 12.3,
      "file_size_kb": 45.2,
      "samples": ["Product 1", "Product 2", "Product 3"]
    },
    ... (8 more)
  }
}
```

## Key Benefits

### âœ… 93% Code Reduction
- **Before:** 9 crawlers Ã— 330 lines = 2,970 lines
- **After:** 1 base crawler (391 lines) + 9 configs (20 lines each) = ~570 lines
- **Savings:** 2,400 lines eliminated!

### âœ… Single Test Run
- Test all 9 crawlers in one command
- No need to run 9 separate tests
- Faster development and CI/CD

### âœ… Unified Reporting
- One Slack notification instead of 9
- Easy to see overall status at a glance
- Compare performance across categories

### âœ… Easy Maintenance
- Update base crawler â†’ all 9 crawlers benefit
- Add new crawler: just URL + category (20 lines)
- Consistent behavior across all categories

## Troubleshooting

### Q: Test fails with "No output file generated"
**A:** Check that:
- Playwright is installed: `playwright install chromium`
- Output directory exists: `mkdir -p test_output/keells`
- Base crawler is imported correctly

### Q: Slack notification not received
**A:** Verify:
- `SLACK_WEBHOOK_URL` secret is set in GitHub repository settings
- Webhook URL is valid and active
- Check GitHub Actions logs for curl output

### Q: Some crawlers fail but others pass
**A:** Check:
- Individual crawler logs in comprehensive test output
- `test_results_summary.json` for error details
- Run failing crawler locally for debugging

### Q: MAX_ITEMS not working
**A:** Ensure:
- Environment variable is set: `MAX_ITEMS=10 pytest ...`
- Base crawler reads `os.getenv("MAX_ITEMS")`
- GitHub Actions passes env var to test step

## Adding a New Crawler

Super easy! Just 2 steps:

### 1. Create crawler file (20 lines)
```python
# crawler/keells/keells_snacks_crawler.py
import asyncio
from keells_base_crawler import crawl_keells_category

async def main(test_mode: bool = False):
    url = "https://www.keellssuper.com/snacks"
    category = "snacks"
    return await crawl_keells_category(url, category, test_mode)

if __name__ == "__main__":
    asyncio.run(main())
```

### 2. Add to test configuration
In `test_all_keells_crawlers.py`, add to `KEELLS_CRAWLERS` dict:
```python
"snacks": {
    "name": "Snacks",
    "url": "https://www.keellssuper.com/snacks",
    "category": "snacks"
}
```

Done! The new crawler will automatically:
- Be tested in the comprehensive test suite
- Appear in the Slack notification
- Generate its own output file

## Next Steps

1. **Push to GitHub** to trigger comprehensive test in CI/CD
2. **Check Slack** for unified notification with all results
3. **Review artifacts** in GitHub Actions for detailed outputs
4. **Monitor performance** by comparing durations across categories

For detailed documentation, see: `COMPREHENSIVE_TEST_GUIDE.md`
